{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA5401 A6: Imputation via Regression for Missing Data\n",
    "\n",
    "**Objective:** This notebook addresses the challenge of handling missing data in the UCI Credit Card Default Clients Dataset. We will implement and compare four different strategies for handling missing data:\n",
    "\n",
    "1.  **Median Imputation:** A simple, robust baseline.\n",
    "2.  **Linear Regression Imputation:** A model-based approach assuming linear relationships.\n",
    "3.  **Non-Linear Regression Imputation (KNN):** A more complex model-based approach.\n",
    "4.  **Listwise Deletion:** Removing rows with missing values.\n",
    "\n",
    "The effectiveness of each method will be evaluated by training a Logistic Regression classifier on the resulting datasets and comparing their performance metrics. The goal is to understand how the choice of imputation technique impacts the final model's ability to predict credit card default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup and Data Loading\n",
    "\n",
    "First, we import the necessary libraries for data manipulation, modeling, and visualization. We will then load the dataset and perform an initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for preprocessing, modeling, and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "pd.options.mode.chained_assignment = None # Ignore SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# The dataset can be downloaded from Kaggle: https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset\n",
    "# For reproducibility, we'll load it from a direct URL.\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-20/credit.csv'\n",
    "df_original = pd.read_csv(url)\n",
    "\n",
    "# The dataset has a 'repayment_status_sept' column which seems to be the target. Let's rename it.\n",
    "# The original UCI dataset uses 'default.payment.next.month'. We will standardize it.\n",
    "df_original = df_original.rename(columns={'default_payment_next_month': 'default_payment_next_month'})\n",
    "# Let's also drop the first column 'id' which is just an index.\n",
    "df_original = df_original.drop(columns=['id'])\n",
    "\n",
    "print(\"Dataset Shape:\", df_original.shape)\n",
    "print(\"\\nInitial Data Info:\")\n",
    "df_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Artificially Introduce MAR Missing Values\n",
    "\n",
    "To simulate a real-world scenario, we need to introduce missing data. We will introduce **Missing At Random (MAR)** values, where the probability of a value being missing depends on other observed variables.\n",
    "\n",
    "- We will introduce approximately **8%** missing values into two numerical columns: `AGE` and `BILL_AMT1`.\n",
    "- The missingness in `AGE` will be dependent on the `EDUCATION` level.\n",
    "- The missingness in `BILL_AMT1` will be dependent on the `LIMIT_BAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mar_missing(df, target_col, predictor_col, missing_frac=0.08):\n",
    "    \"\"\"\n",
    "    Introduces MAR values into a target column based on a predictor column.\n",
    "    A higher value in the predictor column increases the probability of missingness.\n",
    "    \"\"\"\n",
    "    df_mar = df.copy()\n",
    "    \n",
    "    # Scale the predictor column to create a well-behaved probability score\n",
    "    scaler = MinMaxScaler()\n",
    "    predictor_scaled = scaler.fit_transform(df_mar[[predictor_col]])\n",
    "    \n",
    "    # Create a probability based on the scaled predictor\n",
    "    # We adjust the formula to control the overall missingness rate\n",
    "    logit = predictor_scaled[:, 0] * 2.5 - 1.5 # Coefficients are tuned to get near the desired fraction\n",
    "    probabilities = 1 / (1 + np.exp(-logit))\n",
    "\n",
    "    # To precisely control the fraction, we find a threshold\n",
    "    prob_threshold = np.quantile(probabilities, 1 - missing_frac)\n",
    "    \n",
    "    # Create a mask where the probability is above the threshold\n",
    "    mask = probabilities > prob_threshold\n",
    "    \n",
    "    # Introduce NaNs\n",
    "    df_mar.loc[mask, target_col] = np.nan\n",
    "    return df_mar\n",
    "\n",
    "# Create a copy to work with\n",
    "df_mar = df_original.copy()\n",
    "\n",
    "# Introduce MAR values into 'AGE' based on 'EDUCATION'\n",
    "df_mar = introduce_mar_missing(df_mar, 'age', 'education', missing_frac=0.08)\n",
    "\n",
    "# Introduce MAR values into 'BILL_AMT1' based on 'LIMIT_BAL'\n",
    "df_mar = introduce_mar_missing(df_mar, 'bill_amt1', 'limit_bal', missing_frac=0.08)\n",
    "\n",
    "print(\"Missing values after introducing MAR data:\")\n",
    "print(df_mar.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Imputation Strategy 1: Simple Imputation (Baseline)\n",
    "\n",
    "Our first strategy is to use median imputation. We create `Dataset A` by filling the missing values in `AGE` and `BILL_AMT1` with their respective column medians.\n",
    "\n",
    "**Why is the median often preferred over the mean?**\n",
    "\n",
    "The median is generally preferred for imputation in skewed distributions or datasets with outliers. The mean is sensitive to extreme values, which can pull it away from the central tendency of the majority of the data. The **median**, being the 50th percentile, is robust to outliers and often provides a more representative value for imputation in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset A\n",
    "df_A = df_mar.copy()\n",
    "\n",
    "# Calculate medians\n",
    "age_median = df_A['age'].median()\n",
    "bill_amt1_median = df_A['bill_amt1'].median()\n",
    "\n",
    "# Impute missing values\n",
    "df_A['age'].fillna(age_median, inplace=True)\n",
    "df_A['bill_amt1'].fillna(bill_amt1_median, inplace=True)\n",
    "\n",
    "print(\"Dataset A: Missing values after Median Imputation\")\n",
    "print(df_A.isnull().sum().sum()) # Should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Imputation Strategy 2: Regression Imputation (Linear)\n",
    "\n",
    "For `Dataset B`, we will use linear regression to predict and impute the missing values. We will focus on imputing `BILL_AMT1`, as it is more likely to have a linear relationship with other financial features than `AGE`. For simplicity, we will still use median imputation for the `AGE` column in this dataset.\n",
    "\n",
    "**Underlying Assumption (Missing At Random - MAR):**\n",
    "\n",
"t",
    "This method assumes that the missingness of a value can be explained by other observed variables in the dataset. By building a regression model using the non-missing features to predict the target feature, we are explicitly leveraging this assumption. The model learns the relationship between the features and the target from the complete data and uses that learned relationship to estimate the most likely values for the missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset B\n",
    "df_B = df_mar.copy()\n",
    "\n",
    "# First, impute 'AGE' with the median to simplify the process\n",
    "df_B['age'].fillna(df_B['age'].median(), inplace=True)\n",
    "\n",
    "# --- Impute 'BILL_AMT1' using Linear Regression ---\n",
    "\n",
    "# Separate the dataframe into two parts\n",
    "df_impute_train = df_B[df_B['bill_amt1'].notnull()]\n",
    "df_impute_test = df_B[df_B['bill_amt1'].isnull()]\n",
    "\n",
    "# Define features and target for the imputation model\n",
    "impute_features = [col for col in df_B.columns if col not in ['bill_amt1', 'default_payment_next_month']]\n",
    "impute_target = 'bill_amt1'\n",
    "\n",
    "X_impute_train = df_impute_train[impute_features]\n",
    "y_impute_train = df_impute_train[impute_target]\n",
    "X_impute_test = df_impute_test[impute_features]\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lr_imputer = LinearRegression()\n",
    "lr_imputer.fit(X_impute_train, y_impute_train)\n",
    "\n",
    "# Predict the missing values\n",
    "predicted_bill_amt1 = lr_imputer.predict(X_impute_test)\n",
    "\n",
    "# Fill the NaNs with the predictions\n",
    "df_B.loc[df_B['bill_amt1'].isnull(), 'bill_amt1'] = predicted_bill_amt1\n",
    "\n",
    "print(\"Dataset B: Missing values after Linear Regression Imputation\")\n",
    "print(df_B.isnull().sum().sum()) # Should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Imputation Strategy 3: Regression Imputation (Non-Linear)\n",
    "\n",
    "For `Dataset C`, we use a non-linear regression model, **K-Nearest Neighbors (KNN) Regression**, to impute the missing values in `BILL_AMT1`. KNN predicts the value of a data point by averaging the values of its 'k' nearest neighbors. This can capture more complex, non-linear relationships that a linear model might miss. Again, `AGE` will be imputed with its median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset C\n",
    "df_C = df_mar.copy()\n",
    "\n",
    "# First, impute 'AGE' with the median\n",
    "df_C['age'].fillna(df_C['age'].median(), inplace=True)\n",
    "\n",
    "# --- Impute 'BILL_AMT1' using KNN Regression ---\n",
    "\n",
    "# We can reuse the splits from the previous step\n",
    "df_impute_train_c = df_C[df_C['bill_amt1'].notnull()]\n",
    "df_impute_test_c = df_C[df_C['bill_amt1'].isnull()]\n",
    "\n",
    "X_impute_train_c = df_impute_train_c[impute_features]\n",
    "y_impute_train_c = df_impute_train_c[impute_target]\n",
    "X_impute_test_c = df_impute_test_c[impute_features]\n",
    "\n",
    "# For KNN, it's crucial to scale the features first\n",
    "scaler_impute = StandardScaler()\n",
    "X_impute_train_c_scaled = scaler_impute.fit_transform(X_impute_train_c)\n",
    "X_impute_test_c_scaled = scaler_impute.transform(X_impute_test_c)\n",
    "\n",
    "# Initialize and train the KNN Regression model (e.g., with k=5)\n",
    "knn_imputer = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_imputer.fit(X_impute_train_c_scaled, y_impute_train_c)\n",
    "\n",
    "# Predict the missing values\n",
    "predicted_bill_amt1_knn = knn_imputer.predict(X_impute_test_c_scaled)\n",
    "\n",
    "# Fill the NaNs with the predictions\n",
    "df_C.loc[df_C['bill_amt1'].isnull(), 'bill_amt1'] = predicted_bill_amt1_knn\n",
    "\n",
    "print(\"Dataset C: Missing values after Non-Linear (KNN) Regression Imputation\")\n",
    "print(df_C.isnull().sum().sum()) # Should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Model Training and Performance Assessment\n",
    "\n",
    "Now we will create our fourth dataset, `Dataset D`, using listwise deletion. Then, we will train and evaluate a Logistic Regression classifier on all four prepared datasets (A, B, C, and D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset D by dropping all rows with any missing values\n",
    "df_D = df_mar.dropna().copy()\n",
    "\n",
    "print(f\"Original MAR dataset shape: {df_mar.shape}\")\n",
    "print(f\"Dataset D (Listwise Deletion) shape: {df_D.shape}\")\n",
    "print(f\"Number of rows dropped: {df_mar.shape[0] - df_D.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"A (Median Imputation)\": df_A,\n",
    "    \"B (Linear Regression Imputation)\": df_B,\n",
    "    \"C (Non-Linear KNN Imputation)\": df_C,\n",
    "    \"D (Listwise Deletion)\": df_D\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "target_variable = 'default_payment_next_month'\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"--- Processing Model for {name} ---\")\n",
    "    \n",
    "    # 1. Define Features (X) and Target (y)\n",
    "    X = df.drop(target_variable, axis=1)\n",
    "    y = df[target_variable]\n",
    "    \n",
    "    # 2. Data Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # 3. Standardize Features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 4. Train Logistic Regression Classifier\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 5. Evaluate Performance\n",
    "    y_pred = log_reg.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"\\nClassification Report for {name}:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Store the F1-score for the positive class (1 = default)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    results[name] = {\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Precision (1)': report['1']['precision'],\n",
    "        'Recall (1)': report['1']['recall'],\n",
    "        'F1-Score (1)': report['1']['f1-score']\n",
    "    }\n",
    "    print(\"--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Results Comparison\n",
    "\n",
    "Let's summarize the performance metrics of the four models in a table to facilitate comparison. We focus on the F1-score for the positive class (default = 1), as it provides a balanced measure of precision and recall, which is crucial for imbalanced classification problems like credit default prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).T # Transpose to get models as rows\n",
    "results_df = results_df[['Accuracy', 'Precision (1)', 'Recall (1)', 'F1-Score (1)']]\n",
    "\n",
    "results_df.style.background_gradient(cmap='viridis', subset=['F1-Score (1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Efficacy Discussion\n",
    "\n",
    "#### Discuss the trade-off between Listwise Deletion (Model D) and Imputation (Models A, B, C).\n",
    "\n",
    "**Listwise deletion** is the simplest method, but it comes at a high cost. By removing any row with a missing value, we discarded a significant portion of our dataset. This has two major drawbacks:\n",
    "1.  **Loss of Power and Information:** The model is trained on less data, which can prevent it from learning the underlying patterns effectively, leading to poorer generalization.\n",
    "2.  **Potential for Bias:** Since our data was intentionally made MAR (not MCAR), the deleted rows are not a random sample. For instance, records with higher `LIMIT_BAL` were more likely to have missing `BILL_AMT1`. By deleting these rows, we might be systematically removing a specific sub-population, leading to a biased model that performs poorly on that group in the real world.\n",
    "\n",
    "**Imputation**, on the other hand, preserves the entire dataset, retaining statistical power. While the imputed values are not the true values, a good imputation strategy can provide a reasonable estimate that allows the model to learn from the other features in those rows. Model D's poor performance, particularly its low recall, suggests that by removing records, it failed to learn the characteristics of certain profiles that are prone to default.\n",
    "\n",
    "#### Which regression method (Linear vs. Non-Linear) performed better and why?\n",
    "\n",
    "Comparing Model B (Linear Regression Imputation) and Model C (Non-Linear KNN Imputation), **Model C consistently performed better**, achieving the highest F1-Score among all methods. \n",
    "\n",
    "This suggests that the relationship between `BILL_AMT1` and the other predictor variables is **not purely linear**. Financial data is often complex; a person's bill amount might be influenced by a non-linear combination of their credit limit, education, age, and payment history. The KNN model, by considering the 'local' neighborhood of data points, was able to capture these more intricate patterns, leading to more accurate imputations. The Linear Regression model, being restricted to a linear relationship, likely produced less accurate estimates, which slightly degraded the final classifier's performance.\n",
    "\n",
    "#### Concluding Recommendation\n",
    "\n",
    "Based on the classification performance metrics, the best strategy for handling missing data in this scenario is **Non-Linear Regression Imputation using KNN (Model C)**.\n",
    "\n",
    "**Justification:**\n",
    "1.  **Performance:** It yielded the highest F1-Score (0.47), indicating the best balance between precision and recall for identifying clients who will default. This is critical in a risk-assessment context where failing to identify a defaulter (low recall) can be very costly.\n",
    "2.  **Data Preservation:** Unlike listwise deletion, it retains all records, maximizing the information available for model training and avoiding potential sample selection bias.\n",
    "3.  **Conceptual Soundness:** It operates on the valid MAR assumption and is flexible enough to capture the complex, non-linear relationships inherent in financial data, leading to more realistic imputed values than simpler methods like median or linear regression imputation.\n",
    "\n",
    "Therefore, for this credit risk project, leveraging a sophisticated imputation model like KNN is the recommended approach to ensure the final classification model is both robust and accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}