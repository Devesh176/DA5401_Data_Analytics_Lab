{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7d2eeb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h1> <b> <centre> DA5401 Assignment 6 </b> </centre> </h2>\n",
    "<h4> <b> <centre> Name: Pawar Devesh Pramod </centre></b> </h4>\n",
    "<h4> <b> <centre> Roll No: ME22B176  </centre></b> </h4>\n",
    "<h4> <b> <centre> Date of Submission 17/10/2025 </centre></b> </h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4941f2",
   "metadata": {},
   "source": [
    "<h3> <b> Objective: </b> <h3> \n",
    "<p style=\"font-size: 18px;\">This assignment challenges you to apply linear and non-linear regression to impute\n",
    "missing values in a dataset. The effectiveness of your imputation methods will be measured\n",
    "indirectly by assessing the performance of a subsequent classification task, comparing the\n",
    "regression-based approach against simpler imputation strategies. </p>\n",
    "\n",
    "<h3> <b> 1. Problem Statement </b> </h3>\n",
    "<p style=\"font-size: 18 px;\">\n",
    "You are a machine learning engineer working on a credit risk assessment project. You have\n",
    "been provided with the UCI Credit Card Default Clients Dataset. This dataset has missing\n",
    "values in several important feature columns. The presence of missing data prevents the\n",
    "immediate application of many classification algorithms.\n",
    "Your task is to implement three different strategies for handling the missing data and then use\n",
    "the resulting clean datasets to train and evaluate a classification model. This will demonstrate\n",
    "how the choice of imputation technique significantly impacts final model performance.</p>\n",
    "\n",
    "<p style=\"font-size: 18px;\"> Dataset: \n",
    "- <a href=\"https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset/data\">Kaggle - Credit Card\n",
    "Default Clients Dataset</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436a873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The head of dataset:     ID  LIMIT_BAL  SEX  ...  PAY_AMT5  PAY_AMT6  default.payment.next.month\n",
      "0   1    20000.0    2  ...       0.0       0.0                           1\n",
      "1   2   120000.0    2  ...       0.0    2000.0                           1\n",
      "2   3    90000.0    2  ...    1000.0    5000.0                           0\n",
      "3   4    50000.0    2  ...    1069.0    1000.0                           0\n",
      "4   5    50000.0    1  ...     689.0     679.0                           0\n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Null values in the dataset:  ID                            0\n",
      "LIMIT_BAL                     0\n",
      "SEX                           0\n",
      "EDUCATION                     0\n",
      "MARRIAGE                      0\n",
      "AGE                           0\n",
      "PAY_0                         0\n",
      "PAY_2                         0\n",
      "PAY_3                         0\n",
      "PAY_4                         0\n",
      "PAY_5                         0\n",
      "PAY_6                         0\n",
      "BILL_AMT1                     0\n",
      "BILL_AMT2                     0\n",
      "BILL_AMT3                     0\n",
      "BILL_AMT4                     0\n",
      "BILL_AMT5                     0\n",
      "BILL_AMT6                     0\n",
      "PAY_AMT1                      0\n",
      "PAY_AMT2                      0\n",
      "PAY_AMT3                      0\n",
      "PAY_AMT4                      0\n",
      "PAY_AMT5                      0\n",
      "PAY_AMT6                      0\n",
      "default.payment.next.month    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Import the dataset \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('UCI_Credit_Card.csv')\n",
    "print(f'The head of dataset: ', df.head())\n",
    "print(f'Null values in the dataset: ', df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecda30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
       "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default.payment.next.month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00e554",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fb169",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98aa57",
   "metadata": {},
   "source": [
    "### 1. Artificially Introduce MAR Missing Values\n",
    "\n",
    "To simulate a real-world scenario, we need to introduce missing data. We will introduce **Missing At Random (MAR)** values, where the probability of a value being missing depends on other observed variables.\n",
    "\n",
    "- We will introduce approximately **8%** missing values into two numerical columns: `AGE` and `BILL_AMT1`.\n",
    "- The missingness in `AGE` will be dependent on the `EDUCATION` level.\n",
    "- The missingness in `BILL_AMT1` will be dependent on the `LIMIT_BAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50dd5385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after introducing MAR data:\n",
      "ID                               0\n",
      "LIMIT_BAL                        0\n",
      "SEX                              0\n",
      "EDUCATION                        0\n",
      "MARRIAGE                         0\n",
      "AGE                            454\n",
      "PAY_0                            0\n",
      "PAY_2                            0\n",
      "PAY_3                            0\n",
      "PAY_4                            0\n",
      "PAY_5                            0\n",
      "PAY_6                            0\n",
      "BILL_AMT1                     2249\n",
      "BILL_AMT2                        0\n",
      "BILL_AMT3                        0\n",
      "BILL_AMT4                        0\n",
      "BILL_AMT5                        0\n",
      "BILL_AMT6                        0\n",
      "PAY_AMT1                         0\n",
      "PAY_AMT2                         0\n",
      "PAY_AMT3                         0\n",
      "PAY_AMT4                         0\n",
      "PAY_AMT5                         0\n",
      "PAY_AMT6                         0\n",
      "default.payment.next.month       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Intorducing MAR \"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def introduce_mar_missing(df, target_col, predictor_col, missing_frac=0.08):\n",
    "    \"\"\"\n",
    "    Introduces MAR values into a target column based on a predictor column.\n",
    "    A higher value in the predictor column increases the probability of missingness.\n",
    "    \"\"\"\n",
    "    df_mar = df.copy()\n",
    "    \n",
    "    # Scale the predictor column to create a well-behaved probability score\n",
    "    scaler = MinMaxScaler()\n",
    "    predictor_scaled = scaler.fit_transform(df_mar[[predictor_col]])\n",
    "    \n",
    "    # Create a probability based on the scaled predictor\n",
    "    # We adjust the formula to control the overall missingness rate\n",
    "    logit = predictor_scaled[:, 0] * 2.5 - 1.5 # Coefficients are tuned to get near the desired fraction\n",
    "    probabilities = 1 / (1 + np.exp(-logit))\n",
    "\n",
    "    # To precisely control the fraction, we find a threshold\n",
    "    prob_threshold = np.quantile(probabilities, 1 - missing_frac)\n",
    "    \n",
    "    # Create a mask where the probability is above the threshold\n",
    "    mask = probabilities > prob_threshold\n",
    "    \n",
    "    # Introduce NaNs\n",
    "    df_mar.loc[mask, target_col] = np.nan\n",
    "    return df_mar\n",
    "\n",
    "# Create a copy to work with\n",
    "df_mar = df.copy()\n",
    "\n",
    "# Introduce MAR values into 'AGE' based on 'EDUCATION'\n",
    "df_mar = introduce_mar_missing(df_mar, 'AGE', 'EDUCATION', missing_frac=0.08)\n",
    "\n",
    "# Introduce MAR values into 'BILL_AMT1' based on 'LIMIT_BAL'\n",
    "df_mar = introduce_mar_missing(df_mar, 'BILL_AMT1', 'LIMIT_BAL', missing_frac=0.08)\n",
    "\n",
    "print(\"Missing values after introducing MAR data:\")\n",
    "print(df_mar.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6e19d",
   "metadata": {},
   "source": [
    "### 2. Imputation Strategy 1: Simple Imputation (Baseline)\n",
    "\n",
    "Our first strategy is to use median imputation. We create `Dataset A` by filling the missing values in `AGE` and `BILL_AMT1` with their respective column medians.\n",
    "\n",
    "**Why is the median often preferred over the mean?**\n",
    "\n",
    "The median is generally preferred for imputation in skewed distributions or datasets with outliers. The mean is sensitive to extreme values, which can pull it away from the central tendency of the majority of the data. The **median**, being the 50th percentile, is robust to outliers and often provides a more representative value for imputation in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0778c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A: Missing values after Median Imputation\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5284/370899031.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_A['AGE'].fillna(age_median, inplace=True)\n",
      "/tmp/ipykernel_5284/370899031.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_A['BILL_AMT1'].fillna(bill_amt1_median, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset A\n",
    "df_A = df_mar.copy()\n",
    "\n",
    "# Calculate medians\n",
    "age_median = df_A['AGE'].median()\n",
    "bill_amt1_median = df_A['BILL_AMT1'].median()\n",
    "\n",
    "# Impute missing values\n",
    "df_A['AGE'].fillna(age_median, inplace=True)\n",
    "df_A['BILL_AMT1'].fillna(bill_amt1_median, inplace=True)\n",
    "\n",
    "print(\"Dataset A: Missing values after Median Imputation\")\n",
    "print(df_A.isnull().sum().sum()) # Should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299956f8",
   "metadata": {},
   "source": [
    "### 3. Imputation Strategy 2: Regression Imputation (Linear)\n",
    "\n",
    "For `Dataset B`, we will use linear regression to predict and impute the missing values. We will focus on imputing `BILL_AMT1`, as it is more likely to have a linear relationship with other financial features than `AGE`. For simplicity, we will still use median imputation for the `AGE` column in this dataset.\n",
    "\n",
    "**Underlying Assumption (Missing At Random - MAR):**\n",
    "\n",
    "t\n",
    "This method assumes that the missingness of a value can be explained by other observed variables in the dataset. By building a regression model using the non-missing features to predict the target feature, we are explicitly leveraging this assumption. The model learns the relationship between the features and the target from the complete data and uses that learned relationship to estimate the most likely values for the missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec1022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5284/3659660258.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_B['AGE'].fillna(df_B['AGE'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset B: Missing values after Linear Regression Imputation\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create Dataset B\n",
    "df_B = df_mar.copy()\n",
    "\n",
    "# First, impute 'AGE' with the median to simplify the process\n",
    "df_B['AGE'].fillna(df_B['AGE'].median(), inplace=True)\n",
    "\n",
    "# Impute 'BILL_AMT1' using Linear Regression \n",
    "\n",
    "# Separate the dataframe into two parts\n",
    "df_impute_train = df_B[df_B['BILL_AMT1'].notnull()]\n",
    "df_impute_test = df_B[df_B['BILL_AMT1'].isnull()]\n",
    "\n",
    "# Define features and target for the imputation model\n",
    "impute_features = [col for col in df_B.columns if col not in ['BILL_AMT1', 'default_payment_next_month']]\n",
    "impute_target = 'BILL_AMT1'\n",
    "\n",
    "X_impute_train = df_impute_train[impute_features]\n",
    "y_impute_train = df_impute_train[impute_target]\n",
    "X_impute_test = df_impute_test[impute_features]\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lr_imputer = LinearRegression()\n",
    "lr_imputer.fit(X_impute_train, y_impute_train)\n",
    "\n",
    "# Predict the missing values\n",
    "predicted_bill_amt1 = lr_imputer.predict(X_impute_test)\n",
    "\n",
    "# Fill the NaNs with the predictions\n",
    "df_B.loc[df_B['BILL_AMT1'].isnull(), 'BILL_AMT1'] = predicted_bill_amt1\n",
    "\n",
    "print(\"Dataset B: Missing values after Linear Regression Imputation\")\n",
    "print(df_B.isnull().sum().sum()) # Should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab67f0e",
   "metadata": {},
   "source": [
    "### 4. Imputation Strategy 3: Regression Imputation (Non-Linear)\n",
    "\n",
    "For `Dataset C`, we use a non-linear regression model, **K-Nearest Neighbors (KNN) Regression**, to impute the missing values in `BILL_AMT1`. KNN predicts the value of a data point by averaging the values of its 'k' nearest neighbors. This can capture more complex, non-linear relationships that a linear model might miss. Again, `AGE` will be imputed with its median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "723ad6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5284/3711982432.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_C['AGE'].fillna(df_C['AGE'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset C: Missing values after Non-Linear (KNN) Regression Imputation\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Create Dataset C\n",
    "df_C = df_mar.copy()\n",
    "\n",
    "# First, impute 'AGE' with the median\n",
    "df_C['AGE'].fillna(df_C['AGE'].median(), inplace=True)\n",
    "\n",
    "# Impute 'BILL_AMT1' using KNN Regression \n",
    "\n",
    "# We can reuse the splits from the previous step\n",
    "df_impute_train_c = df_C[df_C['BILL_AMT1'].notnull()]\n",
    "df_impute_test_c = df_C[df_C['BILL_AMT1'].isnull()]\n",
    "\n",
    "X_impute_train_c = df_impute_train_c[impute_features]\n",
    "y_impute_train_c = df_impute_train_c[impute_target]\n",
    "X_impute_test_c = df_impute_test_c[impute_features]\n",
    "\n",
    "# For KNN, it's crucial to scale the features first\n",
    "scaler_impute = StandardScaler()\n",
    "X_impute_train_c_scaled = scaler_impute.fit_transform(X_impute_train_c)\n",
    "X_impute_test_c_scaled = scaler_impute.transform(X_impute_test_c)\n",
    "\n",
    "# Initialize and train the KNN Regression model (e.g., with k=5)\n",
    "knn_imputer = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_imputer.fit(X_impute_train_c_scaled, y_impute_train_c)\n",
    "\n",
    "# Predict the missing values\n",
    "predicted_bill_amt1_knn = knn_imputer.predict(X_impute_test_c_scaled)\n",
    "\n",
    "# Fill the NaNs with the predictions\n",
    "df_C.loc[df_C['BILL_AMT1'].isnull(), 'BILL_AMT1'] = predicted_bill_amt1_knn\n",
    "\n",
    "print(\"Dataset C: Missing values after Non-Linear (KNN) Regression Imputation\")\n",
    "print(df_C.isnull().sum().sum()) # Should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b151a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1260ba",
   "metadata": {},
   "source": [
    "## Part B: Model Training and Performance Assessment\n",
    "\n",
    "Now we will create our fourth dataset, `Dataset D`, using listwise deletion. Then, we will train and evaluate a Logistic Regression classifier on all four prepared datasets (A, B, C, and D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e6651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original MAR dataset shape: (30000, 25)\n",
      "Dataset D (Listwise Deletion) shape: (27328, 25)\n",
      "Number of rows dropped: 2672\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset D by dropping all rows with any missing values\n",
    "df_D = df_mar.dropna().copy()\n",
    "\n",
    "print(f\"Original MAR dataset shape: {df_mar.shape}\")\n",
    "print(f\"Dataset D (Listwise Deletion) shape: {df_D.shape}\")\n",
    "print(f\"Number of rows dropped: {df_mar.shape[0] - df_D.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c74d28ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Model for A (Median Imputation) \n",
      "\n",
      "Classification Report for A (Median Imputation):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      4673\n",
      "           1       0.69      0.24      0.36      1327\n",
      "\n",
      "    accuracy                           0.81      6000\n",
      "   macro avg       0.75      0.61      0.62      6000\n",
      "weighted avg       0.79      0.81      0.77      6000\n",
      "\n",
      "*-------------------------------------------------*\n",
      "\n",
      "Processing Model for B (Linear Regression Imputation) \n",
      "\n",
      "Classification Report for B (Linear Regression Imputation):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      4673\n",
      "           1       0.69      0.24      0.36      1327\n",
      "\n",
      "    accuracy                           0.81      6000\n",
      "   macro avg       0.75      0.61      0.62      6000\n",
      "weighted avg       0.79      0.81      0.77      6000\n",
      "\n",
      "*-------------------------------------------------*\n",
      "\n",
      "Processing Model for C (Non-Linear KNN Imputation) \n",
      "\n",
      "Classification Report for C (Non-Linear KNN Imputation):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      4673\n",
      "           1       0.69      0.24      0.36      1327\n",
      "\n",
      "    accuracy                           0.81      6000\n",
      "   macro avg       0.76      0.61      0.62      6000\n",
      "weighted avg       0.79      0.81      0.77      6000\n",
      "\n",
      "*-------------------------------------------------*\n",
      "\n",
      "Processing Model for D (Listwise Deletion) \n",
      "\n",
      "Classification Report for D (Listwise Deletion):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88      4198\n",
      "           1       0.71      0.25      0.37      1268\n",
      "\n",
      "    accuracy                           0.80      5466\n",
      "   macro avg       0.76      0.61      0.62      5466\n",
      "weighted avg       0.79      0.80      0.76      5466\n",
      "\n",
      "*-------------------------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "datasets = {\n",
    "    \"A (Median Imputation)\": df_A,\n",
    "    \"B (Linear Regression Imputation)\": df_B,\n",
    "    \"C (Non-Linear KNN Imputation)\": df_C,\n",
    "    \"D (Listwise Deletion)\": df_D\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "target_variable = 'default.payment.next.month'\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"Processing Model for {name} \")\n",
    "    \n",
    "    # 1. Define Features (X) and Target (y)\n",
    "    X = df.drop(target_variable, axis=1)\n",
    "    y = df[target_variable]\n",
    "    \n",
    "    # 2. Data Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # 3. Standardize Features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 4. Train Logistic Regression Classifier\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 5. Evaluate Performance\n",
    "    y_pred = log_reg.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"\\nClassification Report for {name}:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Store the F1-score for the positive class (1 = default)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    results[name] = {\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Precision (1)': report['1']['precision'],\n",
    "        'Recall (1)': report['1']['recall'],\n",
    "        'F1-Score (1)': report['1']['f1-score']\n",
    "    }\n",
    "    print(\"*-------------------------------------------------*\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2cc05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f6d6a",
   "metadata": {},
   "source": [
    "## Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b183be",
   "metadata": {},
   "source": [
    "### 1. Results Comparison\n",
    "\n",
    "Let's summarize the performance metrics of the four models in a table to facilitate comparison. We focus on the F1-score for the positive class (default = 1), as it provides a balanced measure of precision and recall, which is crucial for imbalanced classification problems like credit default prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3252d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/jinja2/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jinja2\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2)\n",
      "  Using cached markupsafe-3.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d0f9fda3e50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /packages/6f/bc/4dc914ead3fe6ddaef035341fee0fc956949bbd27335b611829292b89ee2/markupsafe-3.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/6f/bc/4dc914ead3fe6ddaef035341fee0fc956949bbd27335b611829292b89ee2/markupsafe-3.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading markupsafe-3.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
      "Installing collected packages: MarkupSafe, jinja2\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [jinja2]2m1/2\u001b[0m [jinja2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 jinja2-3.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7320ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_536e0_row0_col3 {\n",
       "  background-color: #375b8d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_536e0_row1_col3 {\n",
       "  background-color: #471164;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_536e0_row2_col3 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_536e0_row3_col3 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_536e0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_536e0_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_536e0_level0_col1\" class=\"col_heading level0 col1\" >Precision (1)</th>\n",
       "      <th id=\"T_536e0_level0_col2\" class=\"col_heading level0 col2\" >Recall (1)</th>\n",
       "      <th id=\"T_536e0_level0_col3\" class=\"col_heading level0 col3\" >F1-Score (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_536e0_level0_row0\" class=\"row_heading level0 row0\" >A (Median Imputation)</th>\n",
       "      <td id=\"T_536e0_row0_col0\" class=\"data row0 col0\" >0.808500</td>\n",
       "      <td id=\"T_536e0_row0_col1\" class=\"data row0 col1\" >0.689362</td>\n",
       "      <td id=\"T_536e0_row0_col2\" class=\"data row0 col2\" >0.244160</td>\n",
       "      <td id=\"T_536e0_row0_col3\" class=\"data row0 col3\" >0.360601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_536e0_level0_row1\" class=\"row_heading level0 row1\" >B (Linear Regression Imputation)</th>\n",
       "      <td id=\"T_536e0_row1_col0\" class=\"data row1 col0\" >0.808333</td>\n",
       "      <td id=\"T_536e0_row1_col1\" class=\"data row1 col1\" >0.689507</td>\n",
       "      <td id=\"T_536e0_row1_col2\" class=\"data row1 col2\" >0.242653</td>\n",
       "      <td id=\"T_536e0_row1_col3\" class=\"data row1 col3\" >0.358974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_536e0_level0_row2\" class=\"row_heading level0 row2\" >C (Non-Linear KNN Imputation)</th>\n",
       "      <td id=\"T_536e0_row2_col0\" class=\"data row2 col0\" >0.808667</td>\n",
       "      <td id=\"T_536e0_row2_col1\" class=\"data row2 col1\" >0.693305</td>\n",
       "      <td id=\"T_536e0_row2_col2\" class=\"data row2 col2\" >0.241899</td>\n",
       "      <td id=\"T_536e0_row2_col3\" class=\"data row2 col3\" >0.358659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_536e0_level0_row3\" class=\"row_heading level0 row3\" >D (Listwise Deletion)</th>\n",
       "      <td id=\"T_536e0_row3_col0\" class=\"data row3 col0\" >0.801866</td>\n",
       "      <td id=\"T_536e0_row3_col1\" class=\"data row3 col1\" >0.710706</td>\n",
       "      <td id=\"T_536e0_row3_col2\" class=\"data row3 col2\" >0.246057</td>\n",
       "      <td id=\"T_536e0_row3_col3\" class=\"data row3 col3\" >0.365554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x73b97183aaf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "pd.options.mode.chained_assignment = None # Ignore SettingWithCopyWarning\n",
    "results_df = pd.DataFrame(results).T # Transpose to get models as rows\n",
    "results_df = results_df[['Accuracy', 'Precision (1)', 'Recall (1)', 'F1-Score (1)']]\n",
    "\n",
    "results_df.style.background_gradient(cmap='viridis', subset=['F1-Score (1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bf426",
   "metadata": {},
   "source": [
    "### 2. Efficacy Discussion\n",
    "\n",
    "#### Trade-off Between Listwise Deletion and Imputation\n",
    "\n",
    "The primary trade-off is between data preservation and simplicity. Imputation methods (Models A, B, C) retain the full sample size, preserving statistical power and avoiding bias that can occur when data is not missing completely at random. Listwise deletion (Model D) is simple but discards a significant number of observations (5,466 vs. 6,000), which can lead to a less robust and potentially biased model.\n",
    "\n",
    "Interestingly, in this specific experiment, Model D (Listwise Deletion) yielded a slightly higher F1-score (0.37). This unexpected result could happen if the removed rows contained noisy or less informative data, coincidentally leading to a \"cleaner\" dataset for the classifier. However, one should be cautious. Even though it performed marginally better here, Model D could perform poorly in a real-world scenario because it was trained on a smaller, potentially unrepresentative subset of the population, and it may not generalize well to cases similar to those that were deleted.\n",
    "\n",
    "#### Linear vs. Non-Linear Regression Performance\n",
    "\n",
    "The assignment asks to compare the linear and non-linear regression imputation methods. Based on the results, there was no difference in performance between Model B (Linear) and Model C (Non-Linear). Both resulted in an identical F1-score of 0.36.\n",
    "\n",
    "This suggests a few possibilities:\n",
    "\n",
    "- The relationship between the predictors and the imputed feature (BILL_AMT1) might not have been strong enough for a more complex model like KNN to provide significantly better estimates than linear regression.\n",
    "\n",
    "- The imputed feature itself may not have been a dominant predictor for the final classification task. Therefore, even if the non-linear imputations were slightly more accurate, they were not different enough to change the logistic regression model's final predictions.\n",
    "\n",
    "#### Concluding Recommendation\n",
    "\n",
    "Based on the analysis, a recommendation involves balancing performance, conceptual soundness, and practicality.\n",
    "\n",
    "While Model D (Listwise Deletion) had the highest F1-score, it's a risky strategy due to its inherent drawbacks of data loss and potential bias. The three imputation methods all produced identical, stable results.\n",
    "\n",
    "Given that the simple Median Imputation (Model A) performed exactly the same as the more computationally intensive regression-based methods (Models B and C), it stands out as the most pragmatic choice in this scenario.\n",
    "\n",
    "**Recommendation:** The best strategy for handling missing data in this case is Median Imputation.\n",
    "\n",
    "**Justification:** It provides the same predictive performance as more complex regression models while being far simpler and faster to implement. It successfully avoids the data loss and potential bias associated with listwise deletion, making it a robust, efficient, and effective choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA5401",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
